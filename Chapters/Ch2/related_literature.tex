Three different bodies of literature are related to this study. Section 2.1 surveys phoneme-confusion studies from the empirical approach described above, Section 2.2 describes several feature theories from the theoretical approach, and section 2.3 surveys relevant studies about metric learning from the field of Machine Learning. 

\subsection{The empirical approach to phoneme similarity}
The empirical approach to phoneme similarity is based on the assumption that the more confusable two phonemes are the more similar the two are (citation). Several studies provide phoneme-confusion data as were measured in perceptual tasks (citations), working memory tasks (\cite{Wickelgren}, \cite{Wickelgren1965}), spontaneous speech errors (citations - Fromkin), or other tasks(citations). The data is commonly provided in confusion matrices where each value in the matrix represents the number of confusions between a pair of phonemes. From these values, phoneme similarity, or the perceptual distance between phonemes, can then be assessed.

We focus on perceptual tasks only, as we assume that the similarity space of phonemes could be substantially different when considering different systems (perceptual, articulatory, working-memory, etc.), and the related tasks. In particular, we focus on two two previous studies about phoneme confusion in English (cite, NM, \cite{Luce1987}), and data that we have collected for Hebrew phonemes (section 4).

\paragraph{The Nicely and Miller dataset} The classic work by Nicely and Miller (henceforth N \& M) studied auditory confusion between 16 consonant phonemes of English that were added white noise at different signal-to-noise ratios (SNRs). The noise-corrupted phonemes were presented to subjects in a classification-task paradigm, in a consonant-vowel (CV) form, and the affect of low- and high-pass filtering on phoneme confusion was explored. The resulting confusion matrices from the experiments are provided in their paper. In what follows, we refer to this dataset as the \textit{N \& M dataset}. Using tools from information theory, N \& M analysed the confusion matrices to shed lie on the underlying subphonemic features (voicing, nasality, affrication, duration and place of articulation), which were described as separated information-transfer channels. Their results suggest that the voicing and nasality features are less affected by random noise than the other three features: affrication (distinguishes between /fTsSvDzZ/ and /ptkbdgmn/), duration (distinguishes between /sSzZ/ and the other phonemes) and place of articulation (three-valued classification: front, middle and back). Affrication and duration were found to be more affected by random noise, but less than the place of articulation feature, which is the most affected by random noise. 

\paragraph{The Luce dataset} A continuation of the work of N \& M's was carried by Paul Luce. This work extends N \& M work in several aspects. The first is that the segment inventory was extended from 16 to 24 phonemes (see appendix A for the full lists of phonemes). The second aspect is the use of more natural stimuli. The use of words instead of the more artificial CV form of presenting stimuli. The words in the experiments differed by either the initial consonant only, or by the last one. Auditory confusion was explored for these two cases separately. Also, similarly to N \& M, auditory confusion was tested at different levels of SNRs, using white noise. In what follows, we refer to this dataset as the \textit{Luce dataset}. In our study, we focused on the -5 SNR level as it results with the highest rates of confusions. We also used initial-consonant confusion only - (explanation + citation). 

\subsection{Feature theories}
The feature-based theoretical approach to phoneme similarity is based on linguistic or acoustic analyses of phonemes, proposing subphonemic features as the underlying units of phoneme representations. Phoneme similarity can then be derived from, e.g., feature overlap, natural classes, or from a general similarity function defined over features. 

\cite{Clements1985}, \cite{ChomskyHalle1968}, Halle and Clements, Frisch, etc. Articulatory features,  Phonological features..Acoustic features (Mielke).

\subsection{Phoneme-similarity theories}
A similarity theory provides a function to measure similarity between pairs of objects. Given a feature theory, the similarity between all pairs can be evaluated from the theory. \cite{Tversky1977}

A phoneme-similarity theory provides a function to measure similarity between pairs of phonemes. Several phoneme-similarity theories have been proposed, similarity based on features (\cite{ChomskyHalle1968}, Broe...), hierarchical tree (Clements), or on natural classes().

Similarity theories often assume equal contribution of subphonemic features to the overall similarity (citations). Some studies have explored feature weighing based on theoretical considerations (e.g., \cite{Frisch1997}). Discuss feature-based, natural-class based..

\subsection{Metric learning}
Having a theory about the distance, or similarity, between two objects is a fundamental problem in many fields. The goal of Metric Learning is to learn a metric (distance) function over pairs of objects from data. The problem can be addressed through two settings: supervised or unsupervised learning. In the supervised setting, the metric function is learned from pairs of objects whose distance is known, called the \textit{train set}. The generalization of the learning is then evaluated for unseen pairs of objects, called the \textit{test set}. The unsupervised setting of metric learning commonly reduce the data to a lower dimension, using approximation methods (cite PCA, MDS, etc.).

Metric learning is a data-driven approach in its nature. The parameters of the metric function are learned from the training data during the learning procedure. Only the structure of the metric function is assumed. In this study, we test two ways to learn the parameters of the metric function: (1) reducing the problem into a least-squares problem (section 3.2.1), and (2) reducing the problem into a large-margin classification problem \cite{Chechik2010} (section 3.2.2).
